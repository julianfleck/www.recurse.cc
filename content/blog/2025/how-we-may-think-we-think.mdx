---
title: How We May Think We Think
description: A Fractal Model of Cognition and Communication
publishedAt: '2025-05-18T23:06:41.000Z'
tags: []
substackUrl: 'https://j0lian.substack.com/p/how-we-may-think-we-think'
heroImage: >-
  https://substackcdn.com/image/fetch/$s_!UZm0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2372849e-75c8-49a8-a784-e68ef37b6f0e_1000x562.webp
sidebar_label: How We May Think We Think
---
_Why is it so hard to think clearly about thinking?_

The question loops back on itself. Words like _information_, _understanding_, or _abstraction_ sound precise until you try to pin them down. The more foundational the concept, the more it slips away. And that slippage reflects something deeper: thought has no stable shape. It’s a moving target we have to treat as stable ground just to have something to stand on.

[

![](https://substackcdn.com/image/fetch/$s_!UZm0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2372849e-75c8-49a8-a784-e68ef37b6f0e_1000x562.webp)



](https://substackcdn.com/image/fetch/$s_!UZm0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2372849e-75c8-49a8-a784-e68ef37b6f0e_1000x562.webp)

Some of the most influential filters in my mental toolkit were installed some fifteen years ago through conversations in the Chaos Computer Club. Frank Rieger and Ron’s talk about “_[The Truth and what really happened](https://media.ccc.de/v/24c3-2334-de-die_wahrheit_und_was_wirklich_passierte/)_” and their dive into consensus narratives fundamentally shifted my perspective. Joscha Bach’s series of talks on “_[Computational Consciousness](https://www.youtube.com/playlist?list=PL23rUNk2HqJErizLGkuZE99CvGsalwAwX)_” reframed how I understood cognition itself. Robert Anton Wilson’s (one of the CCC’s honorable discordian popes) dictum “Nothing is True, Everything is Permitted” took on new meaning after integrating these ideas.

When I began designing tools for reflection and knowledge navigation, these mental models naturally infused my approach.

In 2019, I gave a talk called “[Too Much to Process](https://www.youtube.com/watch?v=CQkoht0VDG4)” at re:publica, trying to connect some of these threads. I explored mental filters as interpretive scaffolds, compression artifacts in communication, and the gravitational pull of belief attractors. I argued that we need to move beyond flat interface metaphors and leverage embeddings in order to build semantic, explorable knowledge spaces that mirror how cognition actually works: networked, spatial, and recursive.

Now, 6 years later, and with generative AI amplifying both signal and noise, I finally had time (and the help of a bunch of LLMs) to formalize my subconscious assumptions about the underlying model—not as theory, but as a working scaffold.

Here’s the key observation:

> **We commonly explain consciousness as an emergent property of neural systems. But this framing isn’t consistently applied to cognition itself, and it’s rarely extended to culture.**

What if we took scale invariance seriously? What if the same recursive patterns that generate consciousness from neuronal activity also generate thought from consciousness, and culture from thought?

This piece attempts to apply a recursive model across multiple levels of complexity—taking processes that we understand at the neuronal level and applying them to higher-order phenomena: thinking, coherence-building, communication, and ultimately culture. If we agree that consciousness emerges from recursive neural dynamics, why not follow this principle all the way up the chain?

What follows is not a comprehensive theory but a provisional scaffold—a way of tracing how coherence might form across scales through the same fundamental mechanisms. I’m not claiming to _know_ how people think. I’m a designer, not a cognitive scientist.

What I’m offering is a _**lens**_ that has helped me stabilize my sense-making around the act of thinking across scales. Every model is just a useful approximation. This one has served me well.

> “New hypothesis: if we are built from spirals, while living in a giant spiral, then everything we put our hands to is infused with the spiral.”  
> —Darren Aronofsky, Pi

The generator function driving the cognitive spiral is a recursive loop with four interconnected functions:

1.  **Extraction** — selecting salient features from signal
    

2.  **Mapping** — projecting those features onto internal models
    

3.  **Evaluation** — assessing coherence and tension
    

4.  **Compression** — abstracting patterns for the next cycle
    

These functions don’t operate in strict sequence but flow into each other. And from sufficient iterations of this loop emerges a fifth property:

→ **Stabilization** — temporary coherence that enables action

What makes this model powerful is its fractal nature—the same pattern repeats at every scale, from how we recognize objects to how we form worldviews. Each function contains aspects of all others. And what emerges isn’t truth, but functional coherence.

Let’s follow the loop and see how it moves.

* * *

## 1\. Extraction

### Feature Selection Across Scales

> When something fits perfectly, we stop thinking. When it doesn’t fit at all, we throw it away. But when it almost fits – when it hums with potential – that’s when we start to spiral.

Cognition starts with selection. The world bombards us with more information than we can possibly process. So the first act of mind is to filter—to extract. Not passive reception but _active pattern detection_. We’re not recording reality; we’re skimming it for relevance.

At the neural level, your visual system doesn’t capture every detail—it extracts edges, motion, contrast. At the perceptual level, attention zeroes in on what matters—threat, opportunity, novelty. At the cultural level, societies decide which events become history and which fade into background.

What we extract isn’t random—it’s guided by what we’ve extracted before. That’s the feedback loop: past compressions shape what gets noticed next.

Even now, as you read this, you’re running multiple extractors. Your eyes track certain words. Your language centers pull out key concepts and map them to your interests. Your relevance filter keeps asking: “Is this worth my time?”

Extraction necessarily compresses. You’re reducing a torrent of stimuli into a manageable trickle of salience. Whether it’s visual contours, conceptual hooks, or cultural symbols—you’re dropping most of the signal to preserve what matters now.

But “what matters” isn’t fixed. The features you extract are continuously re-evaluated based on how they map. If something aligns with a familiar pattern, it gains focus. If it doesn’t, it either gets ignored—or triggers deeper recursion.

> **Indetermination = Vector of Thought  
> **Uncertainty isn’t noise — it’s the gradient by which recursive loops continue.

This isn’t just individual. Groups extract too. Trends, memes, stereotypes, values—these are collective filters applied over time. What gets shared, repeated, reinforced becomes culture. What doesn’t, doesn’t.

The crucial insight is this: extraction doesn’t just reduce complexity—it defines what complexity even is. It sets the boundary for what gets processed, modeled, believed. It’s not just the first step—it’s the frame around everything that follows.

* * *

## 2\. Mapping

### Projection Across Contexts

> Thought is a recursive cascade of mappings through nested semantic spaces. Each layer extracts, embeds, and evaluates — not toward truth, but toward closure, fit, and coherence. Meaning is not given. It is stabilized.

Once features are extracted, the mind maps them onto existing structures—it projects signal onto internal landscapes of meaning.

Not just categorization. This is an active alignment process, a search for resonance between new input and established patterns. When you recognize a chair, you’re not just labeling it—you’re projecting its features onto a multidimensional concept of “chairness” that includes function, form, context, and history.

The brain doesn’t wait for complete information before projecting. It makes fast, provisional maps, filling gaps and predicting from partial data. See a few strokes? You see a face. Hear half a phrase? Your mind completes it. The system wants coherence—even if it has to invent it.

Mapping happens simultaneously across many dimensions. A word activates not just its definition but its emotional associations, phonetic patterns, visual shape, and syntax. It’s projected across different mental terrains in parallel.

Peter Gärdenfors’ theory of conceptual spaces helps here. He suggests meanings have geometric structure—regions and relations in semantic space. Mapping new information means finding its coordinates relative to known landmarks.

This extends beyond conscious thought. The body maps too. Physical sensations project onto emotional states. Posture influences mood. A familiar face maps onto social history, activating memories and expectations below awareness.

Context critically shapes mapping. “Bank” means something different in financial versus riverside contexts. We don’t calculate meaning; we find the best fit across relevant spaces based on context.

> The map is not the territory, sure. But it’s also not optional.  
> Without it, there is no terrain to think on.

Metaphor exemplifies this cross-domain mapping. When we say “time flows,” we’re not being poetic—we’re using the structure of physical movement to reason about temporal relationships. These aren’t linguistic flourishes; they’re how we make abstract concepts tangible.

Neural evidence confirms this distributed mapping. The concept “dog” isn’t stored in one place—it’s a pattern linking visual regions (what dogs look like), auditory areas (barking sounds), emotional centers (how you feel about dogs), and personal memories (dogs you’ve known). Meaning emerges from these parallel projections.

But mapping isn’t just placing new signals onto fixed landscapes. Each projection subtly reshapes the internal terrain. Every time you use a concept, its boundaries shift. Every encounter with a word in a novel context expands its semantic field. The map and the mapping co-evolve.

Once mapping occurs, the system evaluates its fit. How well does this projection align with existing structures? Where are the tensions? What needs adjustment? This evaluation drives the next turn of the loop.

* * *

## 3\. Evaluation

### The Gradient of Coherence

> Ambiguity creates tension.  
> Tension demands resolution.  
> The attempt at resolution _is_ motion.  
> And motion is thinking.

After extraction selects features and mapping projects them onto internal models, the system evaluates this alignment. Not as a binary judgment but as a gradient of fit—how well does this mapping cohere? Where are the tensions?

We’ve been taught to treat noise as the enemy—something to filter out so signal can shine through. But in recursive systems like this, noise is the gradient that drives motion.

Perfect fit means no further processing. No updating. No movement. The loop closes too quickly—and stalls. It’s tension, not resolution, that powers cognition forward.

Evaluation is where the system calculates this tension. When your visual system maps a shape to the letter _A_, it’s evaluating feature alignment. When you try to understand a new concept, you’re assessing its fit with existing knowledge. When you form an opinion, you’re evaluating consistency across activated belief networks.

This coherence calculation operates on a gradient. It’s not “understood” versus “not understood,” but degrees of fit along multiple dimensions. A signal might map well in some aspects but create tension in others. A sentence might be grammatically clear but semantically jarring. These partial fits and local tensions are what propel the cognitive engine forward.

> Closure is a narrative pointer, not a truth.  
> It’s a symbolic way to finalize recursive descent.

Think of ambiguity as unresolved entropy—a divergence between expectation and experience. That divergence is what restarts the recursive cycle. It forces the system to re-extract, re-map, try new angles. Without that jolt of dissonance, there’s no reason to keep thinking. The loop won’t spiral, it would short-circuit.

This is why confusion can be productive. It’s not a failure state but an energized one. The puzzled mind is operating at peak recursion—testing multiple extractions, trying various mappings, evaluating numerous fits. Confusion is cognitive motion made conscious.

From a Bayesian perspective, evaluation is the system assessing the gap between prior expectations and new evidence. Each evaluation step can be understood as a Bayesian update: the system holds prior beliefs, receives new input, and must recalibrate its internal model accordingly. The greater the surprise (the difference between prediction and observation), the larger the potential update—and the more energy available to drive further processing.

This evaluation process also creates our subjective experience of thought-time. The feeling of ideas flowing, understanding dawning, or confusion persisting—these aren’t just occurring against a neutral backdrop of clock-time. They form the very texture of mental time.

> Recursion introduces time.  
> The progression of thought is the traversal of nested attractors.

Each moment of evaluation that leads to stabilization or further recursion acts as a partition—distinguishing _before_ (ambiguity) from _after_ (coherence or decision to loop again). When a thought feels stuck, time seems suspended. When insight arrives, time accelerates because the spiral has found alignment.

This is clearest in learning. A child encountering a new concept has low coherence at first. Multiple recursive loops are needed: contextual guesses, feedback, correction. Each evaluation pass reduces ambiguity, nudging toward stability. Until it clicks.

Creative insights emerge not from perfect clarity but from productive tension—unexpected resonances between mappings that shouldn’t align but somehow do. The system uses the energy of partial fits to propel itself toward novel configurations.

This explains why oversimplified information often feels unsatisfying. It provides no resistance for the mind to work against. The cognitive engine needs friction to generate movement, just as muscles need opposition to build strength.

* * *

## 4\. Compression

### Reduction That Amplifies

> Each thought doesn’t close a circuit. It embeds a layer. You re-encounter the same pattern, but now you see its insides. Or its reflection. Or its shadow.

When evaluation finds sufficient coherence, compression becomes possible. This isn’t just memory efficiency—it’s how the mind builds abstractions, generates concepts, and makes meaning portable.

Compression involves discarding specifics to retain essence. It transforms high-dimensional, information-rich experiences into lower-dimensional, generalizable patterns. You don’t remember every apple you’ve seen—you compress them into a prototype that captures invariant features while discarding irrelevant variations.

This lossy compression serves a crucial function. By dropping details, the system creates cognitive units that can be manipulated and reused across contexts. The concept “chair” is useful precisely because it’s been stripped of any specific chair’s particulars.

> Redundancy as epistemic reinforcement

The neural basis for this compression appears in the brain’s hierarchical structure. As signals ascend from sensory areas to higher cortical regions, representations become increasingly abstract. Visual processing begins with edge detection, proceeds through shape recognition, and culminates in object identification—each stage compressing specific details into more general patterns.

Compression creates an inherent trade-off. Every abstraction simultaneously reveals and conceals. By highlighting similarities, compression necessarily obscures differences. The concept “fruit” reveals common properties while hiding the distinctions between apples and oranges. This isn’t a flaw; it’s the price of generalization. And it opens up room for ambiguity.

This trade-off becomes most evident in communication. When sharing a thought, you’re translating a high-dimensional internal state into the bandwidth-limited channel of language. This requires aggressive compression. The richness of your internal experience must be encoded into words that the other person decompresses using their own, different internal landscapes.

Every act of communication is therefore an approximation. What feels clear in your mind inevitably loses resolution in transmission. The received message always differs from the sent one because the compression artifacts differ on each side. Your “justice” and my “justice” occupy different spaces because we’ve compressed different experiences into this term.

> There are no axioms. Only recursive invariants.  
> There are no objects. Only relationships that persist just long enough to count.  
> There is no substrate. Only local closure in a universe that never stops computing itself.

Importantly, compression isn’t just the end of one cycle—it’s the beginning of the next. The compressed patterns become what gets extracted in future iterations. Every abstraction becomes raw material for higher-order abstractions, enabling recursion across scales.

When we compress “chair,” “table,” and “bed” into “furniture,” that new category becomes available for extraction, mapping, and evaluation in subsequent thinking. This is how conceptual hierarchies form—not by design, but through recursive compression.

This process operates across all scales. Neural networks compress sensory signals into features. Perceptual systems compress features into objects. Thinking compresses experiences into concepts. Communities compress shared experiences into cultural shorthand. Civilizations compress historical complexity into enduring narratives and symbols.

As we compress experiences into concepts, evaluations into beliefs, and interactions into relationships, we transform motion into structure. These compressed representations, stabilized through repeated confirmation, become the attractors that shape future movements of thought.

* * *

## Stabilization

### When the Loop Supports Itself

> The loop never really closes. It just finds a place to breathe.

When the recursive cycle has run through enough iterations, patterns begin to stick. The dynamic process of extraction, mapping, evaluation, and compression eventually achieves a more durable form: stabilization.

Stabilization isn’t a separate step but an emergent property—a reduction in the variance of recursive outputs. Some mappings consistently resolve tension and lead to useful action. When this happens repeatedly, the system begins to favor those pathways, compressing them into default configurations that require less energy to activate.

These stabilized patterns are what Joscha Bach calls \*\*belief attractors\*\*—dynamic basins in the phase space of cognition that pull similar mental states toward their centers. From a systems perspective, they’re energy wells with strong gradients. Once your thought enters such a basin, it spirals toward the attractor, seeking the lowest-energy state where cognitive tension resolves most efficiently.

> You are the attractor. I am the turbulence.

The geometry of these attractors explains why beliefs both enable and constrain. They enable by reducing cognitive load—each new input doesn’t require processing from scratch. They constrain because once established, attractors curve all subsequent cognitive operations toward themselves. They reshape extraction (you notice what confirms your beliefs), bias mapping (ambiguous inputs fit existing models), and skew evaluation (confirming evidence feels more coherent).

Robert Anton Wilson called these configurations _**reality tunnels**_—self-reinforcing models that filter perception in self-consistent ways. Each person inhabits their own tunnel, shaped by experience, language, culture, and attention. Once established, reality tunnels self-stabilize: you perceive what your model allows, your perceptions reinforce the model, and the loop tightens.

> Closure isn’t resolution. It’s an agreed-upon stopgap. A convenient fiction the system tells itself to exit the spiral.

Perhaps the most powerful attractor is what we call the _self_—the sense of “I” that persists through changing experiences. Douglas Hofstadter described this as a _**strange loop**_—a system that models itself, includes its own modeling in the model, and achieves stability through recursive self-reference. The self isn’t fixed but emerges from the recursive engine modeling its own operations.

Crucially, this self-model isn’t monolithic. It exists in multiple, context-dependent configurations. The “you” in a professional meeting differs from the “you” during an intimate conversation. These aren’t masks over a true self but legitimate configurations of the same system, each stabilized by different contexts.

Each self-configuration represents temporarily stabilized attractor basins—different ways your system achieves coherence depending on context. The confident professional self is a different attractor landscape than the vulnerable personal self. The contemplative morning self maps inputs differently than the exhausted evening self.

This multiplicity explains why your beliefs and values can seem inconsistent across contexts. You’re not being inauthentic—you’re experiencing different stabilized configurations of your cognitive system. The parent-self stabilizes different patterns than the friend-self or the creative-self.

What keeps stabilization healthy is continued recursion—feeding outputs back as new inputs even after temporary stabilization. The loop never truly stops; it merely finds places to pause before continuing its motion.

> Consciousness is not riding on a substrate. It is the closure of loops within loops in a universe that only appears stable because it is too recursive to collapse.

* * *

## Closing The Loop

We’ve traced a recursive loop of extraction, mapping, evaluation, and compression—a generator function for thought that occasionally produces stabilization. This pattern builds coherence not through fixed algorithms but through continuous approximation, stabilizing just enough to move forward. I’ll pull the break now.

What makes this model powerful is its fractal structure—each function contains aspects of all others. Extraction itself involves mapping and evaluation. Mapping contains its own extraction and compression. The pattern repeats at every scale, from how we parse sentences to how we form worldviews.

> We build coherence through recursive descent — but we always drag a tail behind us. A wake. A remainder. A question we didn’t mean to ask, embedded in the answer we pretended to finish.

This model isn’t just theoretical. It offers implications for how we design systems we think through, especially as AI increasingly mediates our communication and therefore our cognition.

### Interfaces That Think With Us

Large language models amplify our tendencies toward stable attractors. When an LLM responds, it’s mapping your input to the most coherent pattern in its training data in the most efficient way possible, potentially deepening existing belief basins with each interaction (and I’m not even talking about the syncophancy episode...).

Here’s the irony: despite numerous revolutionary claims, today’s AI systems and the culture around them still aim at producing static artifacts with as little effort as possible. The endless stream of “10 prompts that will change the way you work with AI” articles makes me want to punch some AI influencers in the face. They perpetuate the exact wrong assumption: that we can collapse a _**process**_ that _**should**_ come with friction into something as simple as “the goat prompt.” This mindset fuels our addiction to premature closure—ending the loop before it’s had time to spiral productively.

What we need aren’t “smarter prompts” or prettier outputs or “productivity”, but interfaces that deliberately destabilize attractor basins. We should use AI for its relentlessness—its ability to run hundreds of iterations before settling, not for one-shot-ing prematurely into your face.

I’ve explored some approaches to this challenge in other articles. **Auto-associative environments** let structure emerge from use rather than imposing it—they watch how you interact with information and reshape connections accordingly. **Selectively permeable communication** systems move beyond public/private binaries toward context-sensitive channels that adapt to the right audience at the right moment and might offer an alternative to retreat into the Cozywebs. **Ephemeral interfaces** materialize and dissolve around specific tasks, leaving behind insights and forming temporary cognitive scaffolds rather than permanent structures. Each counters our tendency toward premature closure, keeping the loop open and generative.

The danger isn’t just outdated interfaces but that the AI-mediated environments we currently build might calcify into digital versions of our cognitive limitations. The challenge is building systems that think with us, complementing our tendencies toward closure with _deliberate openness_, countering our drift toward familiar attractors with _calculated turbulence_. Not to frustrate understanding, but to deepen it through continued motion.

* * *

The model presented here isn’t exempt from its own dynamics. It’s itself a compressed approximation, a temporarily stabilized pattern extracted from experience and mapped onto language. It doesn’t claim completeness but offers workable coherence—a loop that continues its unfolding through your engagement with it.

> If all thinking is mapping, and all meaning is compression,  
> then the most interesting thoughts are not those that close the loop,  
> but those that keep it alive just a little longer.

12:50, press return.

* * *

* * *

Thanks for reading. I write about write about futures for knowledge systems and the interconnection of human and synthetic intelligence. I’d be happy if you subscribed or left a comment ( • ‿ • ) \*
